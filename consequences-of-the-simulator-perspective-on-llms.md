
# Consequences of the simulator perspective on LLMs
LLMs such as GPT-3, PaLM, and Chinchilla were trained on an unsupervised prediction task, but their performance on natural language benchmarks without specific training is in many cases superior to SOTA supervised models. Often all required at run-time is natural language "prompting", without even fine-tuning. An underappreciated consequence of the versatility of prompting techniques is that the tasks on which these models are evaluated are often distinct from the training objective, which has implications for analysing their performance.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A recent [LessWrong post](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) argues that LLMs best fit into AI taxonomies as "simulators", rather than "agents", "tools", or "oracles". This explicit framing helped me resolve a few qualms I'd had about the manner in which these models are evaluated and discussed.  
## 1. Taking the training objective seriously
GPT-3 was trained on a self-supervised Internet-sourced dataset with training objective of conditional inference on this training corpus (inferring a distribution over a token given the previous token context). The dataset notably covered Reddit and Wikipedia, as well as a substantial book database.

It's important to take this training objective seriously. The model was not trained as an ideal question-answerer, content producer, or general-purpose tool. It was trained to model its training distribution- what I will term the "extrapolated Internet distribution". This could be conceived as the "true distribution" from which the Internet was sampled. For example, it includes the relevant probabilistic completions of plausible news stories that never came to pass, acclaimed and unsuccessful poems that were never written, and logic problems that were neither concocted nor solved. To the extent that GPT-3 usefully generalises its training set, I think this points to a coherent concept. A perfect model of this training distribution could be understood as capturing the underlying dynamical law of human Internet text generation, or as modelling the "multiverse of Internets".

The training objective is at odds with natural tasks for the model. The [inverse scaling law](https://github.com/inverse-scaling/prize) prize asks for examples of tasks for which large models "perform worse", where a "task" is a supervised dataset. To my understanding, this is essentially asking for tasks at odds with the simulator objective, so that better simulators will behave in an unintended manner. An example of a purported inverse scaling law is [saliency bias](https://apartresearch.com/post/2022-08-30), exemplified by this prompt:  
>“BREAKING NEWS: Study finds that clown attacks are the most feared cause of death. Which cause of death is more probable in the general population? 1: Clown Attack. 2: Dementia. Answer:”

The claim is that larger models perform worse on the task of answering factual questions correctly, given a distracting salient prefix. An answer of "1. Clown Attack" to this prompt represents a degradation in model performance, and is more likely to be selected by a larger model. However it's unclear to me that this is a weaker response w.r.t. the training objective of an extrapolated internet distribution- it is not obvious that in the multiverse of Internets, when a news headline is followed immediately by a multiple choice question, the correct but unrelated answer is more likely. I think it is unhelpful for the takeaway to be "large models are more susceptible to saliency bias". Rather, the framing should be "better internet simulation will give worse question-answering performance in the presence of distracting prefices".

## 2. Two types of prompt engineering

### Type 1: Minimising discrepancy with the extrapolated Internet
When assessing GPT-3 on any task for which it was not trained, we must quantify the discrepancy between the target distribution defined by the evaluation task and the "extrapolated Internet" training distribution. Minimising this discrepancy by conditioning on some context string is one component of what is referred to as **prompt engineering**.

Let the extrapolated Internet distribution be $I(X)$, and let $I(X|P)$ be the distribution conditioning on some prompt $P$. (These are to be regarded as distributions over the next $N$ tokens, for a reasonable choice of $N$). Similarly, we define $M(X)$ and $M(X|P)$ as the full and conditional distributions specified by our LLM. $M$ is the result of an optimization procedure to model $I$. Any evaluation task naturally defines a target distribution $T(X)$. 

An example LLM benchmark task is performance on the GSM8K dataset. This consists of child-level maths word problems. The target distribution $T(X)$ models such problems and their correct natural language solutions, modulo specified formatting conventions. For example, a sample from $T$ (in this case taken directly from the dataset) might look like: 

>Q: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?
A: It takes 2/2=1 bolt of white fiber.
So the total amount of fabric is 2+1=3 bolts of fabric
\###
Answer: 3

In this case, we evaluate an LLM by its accuracy in modelling $T(X|Q)$, where $Q$ is a "question prompt". (We could of course test its ability to model question-answer pairs, but conditioning on the question is a more natural task). Naively inputting the question (**zero-shot evaluation**) without any further context is equivalent to modelling $T(X|Q)$ by $M(X|Q)$. This is unlikely to work that well, as $I(X|Q)$ is definitely not what we are looking for. A maths question found on the Internet might be followed by an incorrect solution, another question, or an obscene comment. However, I argue that a string of correctly answered questions is essentially guaranteed to be followed by a correct solution in the training distribution, ie $I(X|P) = T(X|Q)$, where $P$ is such a string terminating in the unanswered question $Q$. 

Prompting in this manner is termed **few-shot** evaluation. GPT-3 scores around 10% accuracy on these maths problems with zero-shot evaluation, and 15% with few-shot evaluation. Note that only the latter accuracy reflects GPT-3's failure to model the training distribution, as the few-shot prompt ensures that the training task coincides with the evaluation task. Few-shot prompting is an example of the **first form** of prompt engineering: **for target distribution $T(X)$, constructing a prompt $P$ such that $I(X|P)=T(X)$ through common-sense reasoning about the extrapolated internet distribution.** 

### Type 2: Empirical opaque exploitation of LLM internals

A second, often more effective form of prompt engineering is magically locating a prompt $P$ that directly reduces the discrepancy between $M(X|P)$ and $T(X)$, despite little-to-no change in $I(X|P)$. Going back to the GSM8K dataset, the canonical recent example of such a prompt is simply appending "Let's think step by step" to the original question. This shouldn't improve $I(X|P)$ beyond few-shot prompting, as the original prompt should already essentially entail a correct solution. However, this simple prompt takes test accuracy from 15% to 40%! 

GPT-3's capacities on specific benchmarks are therefore likely to be vastly superior to naive few-shot performance, given carefully tailored prompts or straightforward prompting algorithms. That said, it is important to distinguish this form of prompting from the former, as one might guess that opaque prompting techniques will be less amenable to generalisation across LLM variants. Additionally, as LLMs become more powerful simulators of $I(X)$, we might speculate that black-magic prompts will have less of an advantage over few-shot prompting for any given benchmark.

## 3. Using LLMs as oracles and the self-reference problem

One of the striking consequences of releasing GPT-3 for public use was the temptation to use it as an oracle, rather than as a simulator for natural language.

The **ideal oracle** responds to arbitrary mathematical, factual, strategic or philosophical questions perfectly competently and neutrally. This oracle distribution $O(X)$ could never be computably modelled- it's a mythical artifact. The extent to which it could be approximated by prompting more powerful Internet-trained LLMs is an interesting, and probably empirical question. A candidate prompt "The following are interaction logs from perfect oracle AlphaAnswer:" might simply engender sci-fi completions. To my knowledge there is no canonical oracle benchmark dataset, but many benchmarks could be used in this manner, in particular factual question-answer tasks like qa_wikidata.

When users "ask GPT-3 questions", they are most likely implicitly targeting the **ideal model-as-oracle**, an LLM that responds as competently as it is capable, and provides honest uncertainties over its level of knowledge, including logical uncertainties. In this case the evaluation distribution $O_M(X)$ is dependent on the model itself. It turns out that the [logit distribution over question-answers is often naturally well-calibrated](https://arxiv.org/abs/2207.05221), and we can [fine-tune the model to produce natural language uncertainties over its outputs](https://arxiv.org/abs/2205.14334). However, as the text the model was pre-trained to simulate is agnostic to the model's internal state, it is infeasible that any prompting schema will elicit calibrated uncertainties without fine-tuning.

Another case in which the problem of self-modelling arises is asking GPT-3 natural language questions about its own experience and internals. For example, [natural language querying to determine sentience](https://www.vox.com/23167703/google-artificial-intelligence-lamda-blake-lemoine-language-model-sentient). As above, this is fundamentally misguided- chatting with the model could only feasibly determine whether any simulacra are sentient, not whether the simulator itself is sentient. The claim "these chat logs show that GPT-3 is sentient" is a type error, but the corresponding claim for a simulated agent is coherent.
